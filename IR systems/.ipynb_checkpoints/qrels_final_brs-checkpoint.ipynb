{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import sys\n",
    "from num2words import num2words\n",
    "from math import *\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_list ={}\n",
    "# file_name = sys.argv[1]\n",
    "# file_name+=\".txt\"\n",
    "# query = open(file_name,'r')\n",
    "query = open('qrels_question.txt','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in query:\n",
    "    new_text = q.split(\"\\t\")\n",
    "    query_list[new_text[1].replace(\"\\n\",\"\")] = new_text[0].replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj=PorterStemmer()\n",
    "stop_words=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(querytext):\n",
    "    text=str(np.char.lower(querytext))\n",
    "    new_text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "    token_list=word_tokenize(new_text)\n",
    "    word_list=token_list\n",
    "    for ind,word in enumerate(word_list):\n",
    "        if word.isdigit() and len(word)<4:\n",
    "            word_list[ind]=num2words(int(word))\n",
    "\n",
    "    word_list=' '.join(word_list)\n",
    "    word_list=re.sub(r'[^\\w\\s]',' ',word_list)\n",
    "    word_list=word_tokenize(word_list)\n",
    "    word_list=[word for word in word_list if word not in stop_words]\n",
    "    new_words=list(filter(lambda x:len(x)>1,word_list))\n",
    "    stem_words=[obj.stem(word) for word in new_words]\n",
    "    return stem_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('postingLists',\"rb\")\n",
    "posting_lists=pickle.load(file)\n",
    "\n",
    "file=open('fileIndex','rb')\n",
    "file_index=pickle.load(file)\n",
    "\n",
    "file=open(\"fileWordList\",'rb')\n",
    "file_words=pickle.load(file)\n",
    "\n",
    "master_unique_words=set(posting_lists.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_list = {}\n",
    "rel_list = {}\n",
    "for k,v in query_list.items():\n",
    "    master_list[k]=[]\n",
    "    rel_list[k]=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in query_list.items():\n",
    "    query_text=key\n",
    "    words = preprocess(query_text)\n",
    "    # print(words_new)\n",
    "\n",
    "\n",
    "    query_words=[words[0]]\n",
    "#     words_new[0]\n",
    "\n",
    "    for i in range(1,len(words)):\n",
    "        if words[i] not in [\"and\",\"or\"]:\n",
    "            if query_words[-1] not in [\"and\",\"or\"]:\n",
    "                query_words.append(\"and\")\n",
    "                query_words.append(words[i])\n",
    "            else:\n",
    "                query_words.append(words[i])\n",
    "        elif query_words[-1] not in [\"and\",\"or\"]:\n",
    "            query_words.append(words[i])\n",
    "\n",
    "    operators=[]\n",
    "    non_operators=[]\n",
    "\n",
    "    for word in query_words:\n",
    "        value = word.lower()\n",
    "        if value == \"and\" or value == \"or\":\n",
    "            operators.append(value)\n",
    "        else:\n",
    "            non_operators.append(value)\n",
    "    \n",
    "    stemmed_non_operators=[obj.stem(word) for word in non_operators]\n",
    "\n",
    "    tot_doc_size=len(file_index)\n",
    "    vector=[]\n",
    "    word_matrix=[]\n",
    "\n",
    "    for word in stemmed_non_operators:\n",
    "        vector=[0]*tot_doc_size\n",
    "        if word in master_unique_words:\n",
    "            for w in posting_lists[word].keys():\n",
    "                vector[w]=1\n",
    "        word_matrix.append(vector)\n",
    "    \n",
    "\n",
    "    matrix = word_matrix\n",
    "    for op in operators:\n",
    "        first_vector=word_matrix[0]\n",
    "        second_vector=word_matrix[1]\n",
    "        \n",
    "        if op==\"and\":\n",
    "            output=[] \n",
    "            for a,b in zip(first_vector,second_vector):\n",
    "                output.append(a&b)\n",
    "\n",
    "            matrix.pop(0)\n",
    "            matrix.pop(0)        \n",
    "            matrix.insert(0,output)\n",
    "        else:\n",
    "            output=[]\n",
    "            for a,b in zip(first_vector,second_vector):\n",
    "                output.append(a|b) \n",
    "\n",
    "            matrix.pop(0)\n",
    "            matrix.pop(0)        \n",
    "            matrix.insert(0,result)\n",
    "\n",
    "        word_matrix=matrix\n",
    "\n",
    "    cnt=0\n",
    "    # tracker = 10\n",
    "    final_vector = word_matrix[0]\n",
    "    for bit in final_vector:\n",
    "        # if tracker == 0:\n",
    "        #     break\n",
    "        \n",
    "        if bit==1:\n",
    "            master_list[key].append(file_index[cnt][0])\n",
    "            rel_list[query_text].append(1)\n",
    "        else:\n",
    "            rel_list[query_text].append(0)\n",
    "        cnt+=1\n",
    "        # tracker-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['queryid','iteration','docid','relevence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "the_list = []\n",
    "for i in range(4):\n",
    "    the_list.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v, in query_list.items():\n",
    "    docid = v\n",
    "    ind = 0\n",
    "    for doc_name in master_list[k]:\n",
    "        the_list[0].append(v)\n",
    "        the_list[1].append(1)\n",
    "        the_list[2].append(doc_name.split('.')[0])\n",
    "        the_list[3].append(rel_list[k][ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['queryid'] = the_list[0]\n",
    "df['iteration'] = the_list[1]\n",
    "df['docid'] = the_list[2]\n",
    "df['relevence'] = the_list[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(columns=['queryid','iteration','docid','relevence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "for i in new_df['queryid'].unique():\n",
    "    x = new_df[new_df['queryid']==i].head(10)\n",
    "    all_dfs.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in all_dfs:\n",
    "    df.columns = ['queryid','iteration','docid','relevence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_df = pd.concat(all_dfs).reset_index(drop=True)\n",
    "fin_df.to_csv('QRels_brs.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e16b9bd33e7041c0842013505aab8d96e59e427e4e0c80436ba1b7c1f34f93b5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
