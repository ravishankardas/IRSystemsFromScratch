{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d098a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary files\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from num2words import num2words\n",
    "from math import *\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9943e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c53eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiliasing useful libraries for stemming and stop word removal\n",
    "obj = PorterStemmer()\n",
    "stop_words=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759ec5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will convert the number to words\n",
    "def numToWords(words):\n",
    "    for i,w in enumerate(words):\n",
    "        if w.isdigit() and len(w)<4:\n",
    "            words[i]=num2words(int(w))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f6193",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_files={}#this will store the all the preprocessed words at every document index\n",
    "file_index={} #this will store the filename and length of the document in words at every document index\n",
    "master_unique_words=set() #this set will contain all the unique words in whole of the corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c34e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we will preprocess the document by performing certain operations such as tokenization, stemming , making all the\n",
    "#charaters smaller case , removing non-ascii characters, making numbers to words and stpring all the relevent things\n",
    "#in the above mentioned dictionaries and set\n",
    "def preprocess(file_name,idx):\n",
    "   \n",
    "    file=open(\"english-corpora/\"+file_name,'r',encoding='utf-8')#reading the file\n",
    "    text=file.read()\n",
    "    text=text.replace(\"\\n\",\" \")#removing the newline character\n",
    "    text=text.replace(\"\\t\",\" \") #removing the tab character\n",
    "    encoded = text.encode(\"ascii\", \"ignore\")#removing the ascii characters\n",
    "    decoded = encoded.decode()   \n",
    "    text_lower=str(np.char.lower(decoded))#making characters lower case    \n",
    "    text_new=text_lower.replace(\"'\",\"\")#removing the apostrophe \n",
    "    text_new = re.sub(r'[^\\w\\s]',' ',text_new)\n",
    "    token_list=word_tokenize(text_new) #tokenization\n",
    "    word_list=numToWords(token_list)\n",
    "    word_list=' '.join(word_list)\n",
    "    word_list=re.sub(r'[^\\w\\s]',' ',word_list)\n",
    "    word_list=word_tokenize(word_list)\n",
    "    word_list=[x for x in word_list if x not in stop_words]#stopwords removal\n",
    "    words_list_new=list(filter(lambda x:len(x)>1,word_list))    \n",
    "    stem_words = [obj.stem(w) for w in words_list_new]#stemming \n",
    "    master_unique_words.update(stem_words)\n",
    "    \n",
    "    overall_files[idx]=stem_words\n",
    "    file_index[idx]=[file_name,len(stem_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca66abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=0\n",
    "folder=\"english-corpora/*\"#the english corpora folder should be in the same directory as this notebook\n",
    "for file in glob.glob(folder):\n",
    "    print(file)\n",
    "    file_name=os.path.basename(file)\n",
    "    preprocess(file_name,idx)\n",
    "    idx+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d5dd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(master_unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda92030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this dictionary stores for every word which document it is present in and the frequency in that document\n",
    "posting_lists={}\n",
    "for word in master_unique_words:\n",
    "    posting_lists[word]={}#here we intialising with empty structure so that we dont get keyerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71892633",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will return the count of word here words passed is a list and will return a dictionary\n",
    "def find_words_freq_doc(words):\n",
    "    freq={}\n",
    "    #here we are making a set so that we dont count a word more than once\n",
    "    unique_words=set(words)\n",
    "    \n",
    "    #here we are counting the frequency for each word in the passed list\n",
    "    for word in unique_words:\n",
    "        cnt=0\n",
    "        for i in words:\n",
    "            if i == word:\n",
    "                cnt+=1\n",
    "        freq[word]=cnt\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44fc583",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we are iterating the overall_files and finding their frequency and storing them in the posting list\n",
    "for docid in overall_files.keys():\n",
    "    words=overall_files[docid]\n",
    "    \n",
    "    word_freq=find_words_freq_doc(words)\n",
    "    \n",
    "    for word in word_freq.keys():\n",
    "        posting_lists[word][docid]=word_freq[word]#here we are storing the word count for every word present in the document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56295fc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#this dictionary will store the norms which will be used later\n",
    "norm_docs={}\n",
    "total_doc_size=len(file_index)\n",
    "for file in overall_files.keys():\n",
    "    words_list=overall_files[file]\n",
    "    \n",
    "    norm=0\n",
    "    for word in set(words_list):\n",
    "        tf=0 #for calculating term freuency\n",
    "        for i in word_list:\n",
    "            if i==word:\n",
    "                tf+=1\n",
    "        doc_freq=len(posting_lists[word])#total document frency in the document\n",
    "        \n",
    "        idf=log2((total_doc_size+1)/(doc_freq))#calculating IDF\n",
    "        \n",
    "        tf_idf=tf*idf\n",
    "        \n",
    "        norm+=tf_idf**2\n",
    "    \n",
    "    norm_value=sqrt(norm)#calculating norm\n",
    "    norm_docs[file]=norm_value\n",
    "    print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414d74aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"posting_lists\",\"wb\") as f:\n",
    "    pickle.dump(posting_lists,f)\n",
    "\n",
    "with open(\"file_index\",\"wb\") as f:\n",
    "    pickle.dump(file_index,f)\n",
    "\n",
    "with open(\"file_word_list\",\"wb\") as f:\n",
    "    pickle.dump(overall_files,f)\n",
    "    \n",
    "with open(\"file_norms\",\"wb\") as f:\n",
    "    pickle.dump(norm_docs,f)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
